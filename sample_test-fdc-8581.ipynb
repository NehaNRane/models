{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac69471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/9d/a5a5422e00c034e16c8ad2e0361cf26578a8fcbbc45c6d605d2decf72bc7/TPOT-0.12.2-py3-none-any.whl\n",
      "Collecting cloudpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl\n",
      "Collecting mlflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/d9/6ec0b635fcd0eb46d5a671bb5f350defb656a3b925bd780786f9fac0da12/mlflow-2.15.0-py3-none-any.whl (26.3MB)\n",
      "\u001b[K     |████████████████████████████████| 26.3MB 5.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fosforio[snowflake]\n",
      "  Downloading https://files.pythonhosted.org/packages/5b/98/b5e8b5febc280807ac2ebabd7d8f1e1986add138a393fd03647115a200ad/fosforio-1.0.2-py3-none-any.whl\n",
      "Collecting tqdm>=4.36.1\n",
      "  Using cached https://files.pythonhosted.org/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl\n",
      "Collecting scikit-learn>=1.4.1\n",
      "  Using cached https://files.pythonhosted.org/packages/12/f8/d6bc56d20c60b2b66e005dfed16fdf8c46dbe559d40896d06c448a65f134/scikit_learn-1.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting xgboost>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/91/fdbd611bfa9a2c91001ccbbc32b3717d370551a98d083fd0a0bbfcb4b537/xgboost-2.1.1-py3-none-manylinux2014_x86_64.whl\n",
      "Collecting joblib>=0.13.2\n",
      "  Using cached https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl\n",
      "Processing /home/mosaic-ai/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7/stopit-1.1.2-cp39-none-any.whl\n",
      "Collecting numpy>=1.16.3\n",
      "  Using cached https://files.pythonhosted.org/packages/b1/e3/24d289c5a3255bf52824bd52295e9a7923cad8ae5ec29539fc971e1122f6/numpy-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting scipy>=1.3.1\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f5/d0ad1a96f80962ba65e2ce1de6a1e59edecd1f0a7b55990ed208848012e0/scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting pandas>=0.24.2\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/30/f6f1f1ac36250f50c421b1b6af08c35e5a8b5a84385ef928625336b93e6f/pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting deap>=1.2\n",
      "  Using cached https://files.pythonhosted.org/packages/23/45/90610df78aedf2922cbb8ba8bf0683c88e4ed565ccd53d62ea178961fcb0/deap-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting update-checker>=0.16\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
      "Collecting Flask<4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/80/ffe1da13ad9300f87c93af113edd0638c75138c42a0994becfacac078c06/flask-3.0.3-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 28.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser<2\n",
      "  Downloading https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl\n",
      "Collecting graphene<4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/70/96f6027cdfc9bb89fc07627b615cb43fb1c443c93498412beaeaf157e9f1/graphene-3.3-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 81.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow<16,>=4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/9d/b5d4666d78bab32b80275d86043a127d2197431c711a53291b5c94637844/pyarrow-15.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4MB)\n",
      "\u001b[K     |████████████████████████████████| 38.4MB 71.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic!=1.10.0,<2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ed/c884465c33c25451e4a5cd4acad154c29e5341e3214e220e7f3478aa4b0d/alembic-1.13.2-py3-none-any.whl (232kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 81.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown<4,>=3.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/b3/0c0c994fe49cd661084f8d5dc06562af53818cc0abefaca35bdc894577c3/Markdown-3.6-py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 87.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn<23; platform_system != \"Windows\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/97/6d610ae77b5633d24b69c2ff1ac3044e0e565ecbd1ec188f02c45073054c/gunicorn-22.0.0-py3-none-any.whl (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 17.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mlflow-skinny==2.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/0e/4dab2a4a1eba4f05de80cf7f21e5ac166dc4ee338aaf6878d1b3c570c0e4/mlflow_skinny-2.15.0-py3-none-any.whl (5.5MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5MB 54.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2<4,>=2.11; platform_system != \"Windows\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 84.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib<4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/6d/45837c5b3d0005a5a9b04729b218a16bf3aa195701c6b33b2cc39ae943b6/matplotlib-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3MB)\n",
      "\u001b[K     |████████████████████████████████| 8.3MB 75.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy<3,>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/47/9dc7ffea9b544f3f37a4f977cf0a740ff86da49eeb3ba5a3a10b9abe891c/SQLAlchemy-2.0.32-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 72.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker<8,>=4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/26/57c6fb270950d476074c087527a558ccb6f4436657314bfb6cdf484114c4/docker-7.1.0-py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 46.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-connector-python[pandas]==3.6.0; extra == \"snowflake\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/a8/6ffd1f90333dec9a6e08e97894a05a8c2aec56b1e0b73b7a3e21d47d6269/snowflake_connector_python-3.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "Collecting requests>=2.3.0\n",
      "  Using cached https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl\n",
      "Collecting click>=8.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 28.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting blinker>=1.6.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/2a/10164ed1f31196a2f7f3799368a821765c62851ead0e630ab52b8e14b4d0/blinker-1.8.2-py3-none-any.whl\n",
      "Collecting importlib-metadata>=3.6.0; python_version < \"3.10\"\n",
      "  Downloading https://files.pythonhosted.org/packages/82/47/bb25ec04985d0693da478797c3d8c1092b140f3a53ccb984fbbd38affa5b/importlib_metadata-8.2.0-py3-none-any.whl\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Werkzeug>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/6e/e792999e816d19d7fcbfa94c730936750036d65656a76a5a688b57a656c4/werkzeug-3.0.3-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 81.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting aniso8601<10,>=8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/04/e97c12dc034791d7b504860acfcdd2963fa21ae61eaca1c9d31245f812c3/aniso8601-9.0.1-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 16.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphql-relay<3.3,>=3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/74/16/a4cf06adbc711bd364a73ce043b0b08d8fa5aae3df11b6ee4248bcdad2e0/graphql_relay-3.2.0-py3-none-any.whl\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/39/e5143e7ec70939d2076c1165ae9d4a3815597019c4d797b7f959cf778600/graphql_core-3.2.3-py3-none-any.whl (202kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 80.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Mako\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/62/70f5a0c2dd208f9f3f2f9afd103aec42ee4d9ad2401d78342f75e9b8da36/Mako-1.3.5-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4\n",
      "  Downloading https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl\n",
      "Collecting packaging\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 18.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-api<3,>=1.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/a7/6322d1d7a1fb926e8b99208c27730f21217da2f1e0e11dab48a78a0427a4/opentelemetry_api-1.26.0-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 21.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting databricks-sdk<1,>=0.20.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/69/1bdc016ebdec92770728f2e233ce09e233a04bd3352ec58c4f08619c1acf/databricks_sdk-0.29.0-py3-none-any.whl (505kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 77.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<6,>=3.12.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/98/db690e43e2f28495c8fc7c997003cbd59a6db342914b404e216c9b6791f0/protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 78.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting entrypoints<1\n",
      "  Downloading https://files.pythonhosted.org/packages/35/a8/365059bbcd4572cbc41de17fd5b682be5868b218c3c5479071865cab9078/entrypoints-0.4-py3-none-any.whl\n",
      "Collecting pyyaml<7,>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/39/472f2554a0f1e825bd7c5afc11c817cd7a2f3657460f7159f691fbb37c51/PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 71.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse<1,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/a5/b2860373aa8de1e626b2bdfdd6df4355f0565b47e51f7d0c54fe70faf8fe/sqlparse-0.5.1-py3-none-any.whl (44kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 18.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitpython<4,>=3.1.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl (207kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 79.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-sdk<3,>=1.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/f1/a9b550d0f9c049653dd2eab45cecf8fe4baa9795ed143d87834056ffabaf/opentelemetry_sdk-1.26.0-py3-none-any.whl (109kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 85.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=5.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/04/e6/a1551acbaa06f3e48b311329828a34bc9c51a8cfaecdeb4d03c329a1ef85/cachetools-5.4.0-py3-none-any.whl\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/5a/360da85076688755ea0cceb92472923086993e86b5613bbae9fbc14136b0/MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting contourpy>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/a2/2f12e3a6e45935ff694654b710961b03310b0e1ec997ee9f416d3c873f87/contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 81.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
      "  Downloading https://files.pythonhosted.org/packages/75/06/4df55e1b7b112d183f65db9503bff189e97179b256e1ea450a3c365241e0/importlib_resources-6.4.0-py3-none-any.whl\n",
      "Collecting pillow>=8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/3f/c02268d0c6fb6b3958bdda673c17b315c821d97df29ae6969f20fb49388a/pillow-10.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4MB 76.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl\n",
      "Collecting fonttools>=4.22.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/30/ad4483dfc5a1999f26b7bc5edc311576f433a3e00dd8aea01f2099c3a29f/fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6MB 81.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/a8/841594f11d0b88d8aeb26991bc4dac38baa909dc58d0c4262a4f7893bcbf/kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 82.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/ea/6d76df31432a0e6fdf81681a895f009a4bb47b3c39036db3e1b528191d52/pyparsing-3.1.2-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 78.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet!=0.4.17; python_version < \"3.13\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/ea/8bc7ed08ba274bdaff08f2cb546d832b8f44af267e03ca6e449840486915/greenlet-3.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660kB)\n",
      "\u001b[K     |████████████████████████████████| 665kB 80.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3>=1.26.0\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/1c/89ffc63a9605b583d5df2be791a27bc1a42b7c32bab68d3c8f2f73a98cd4/urllib3-2.2.2-py3-none-any.whl\n",
      "Collecting cffi<2.0.0,>=1.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/ac/e9e77bc385729035143e54cc8c4785bd480eaca9df17565963556b0b7a93/cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 77.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyOpenSSL<24.0.0,>=16.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/de/007b832ad7a95e6a73745609bbe123c407aa2c46bb0b8f765c8718294e7f/pyOpenSSL-23.3.0-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 15.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=2.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached https://files.pythonhosted.org/packages/98/69/5d8751b4b670d623aa7a47bef061d69c279e9f922f6705147983aa76c3ce/charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tomlkit\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/7c/b753bf603852cab0a660da6e81f4ea5d2ca0f0b2b4870766d7aa9bceb7a2/tomlkit-0.13.0-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/d5/c84e1a17bf61d4df64ca866a1c9a913874b4e9bdc131ec689a0ad013fb36/certifi-2024.7.4-py3-none-any.whl\n",
      "Collecting filelock<4,>=3.5\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/f0/48285f0262fe47103a4a45972ed2f9b93e4c80b8fd609fa98da78b2a5706/filelock-3.15.4-py3-none-any.whl\n",
      "Collecting platformdirs<4.0.0,>=2.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl\n",
      "Collecting asn1crypto<2.0.0,>0.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/7f/09065fd9e27da0eda08b4d6897f1c13535066174cc023af248fc2a8d5e5a/asn1crypto-1.5.1-py2.py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 86.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyjwt<3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/79/84/0fdf9b18ba31d69877bd39c9cd6052b47f3761e9910c15de788e519f079f/PyJWT-2.9.0-py3-none-any.whl\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/3e/741d8c82801c347547f8a2a06aa57dbb1992be9e948df2ea0eda2c8b79e8/idna-3.7-py3-none-any.whl\n",
      "Collecting cryptography<42.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/fd/dd5bd6ab0d12476ebca579cbfd48d31bd90fa28fa257b209df585dcf62a0/cryptography-41.0.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4MB 76.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/20/38/f5c473fe9b90c8debdd29ea68d5add0289f1936d6f923b6b9cc0b931194c/zipp-3.19.2-py3-none-any.whl\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl\n",
      "Collecting google-auth~=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/00/85c22f7f73fa2e88dfbf0e1f63c565386ba40e0264b59c8a4362ae27c9fc/google_auth-2.32.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 80.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 15.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.47b0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/c2/ca5cef8e4cd8eec5a95deed95ec3f6005e499fd9d17ca08731ced03a6921/opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 73.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl (117kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 77.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt<2,>=1.10\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/e7/459a8a4f40f2fa65eb73cb3f339e6d152957932516d18d0e996c7ae2d7ae/wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 22.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/68/8906226b15ef38e71dc926c321d2fe99de8048e9098b5dfd38343011c886/pyasn1_modules-0.4.0-py3-none-any.whl (181kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 83.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/7e/5f50d07d5e70a2addbccd90ac2950f81d1edd0783630651d9268d7f1db49/pyasn1-0.6.0-py2.py3-none-any.whl (85kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 21.0MB/s eta 0:00:01\n",
      "\u001b[31mERROR: openapi-schema-validator 0.6.2 has requirement jsonschema<5.0.0,>=4.19.1, but you'll have jsonschema 4.19.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: numba 0.58.1 has requirement numpy<1.27,>=1.22, but you'll have numpy 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 3.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement itsdangerous==2.0.1, but you'll have itsdangerous 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Jinja2==3.0.3, but you'll have jinja2 3.1.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.7.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jsonschema-path 0.3.2 has requirement referencing<0.32.0,>=0.28.0, but you'll have referencing 0.33.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyarrow 15.0.2 has requirement numpy<2,>=1.16.6, but you'll have numpy 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: opentelemetry-api 1.26.0 has requirement importlib-metadata<=8.0.0,>=6.0, but you'll have importlib-metadata 8.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow-skinny 2.15.0 has requirement importlib-metadata!=4.7.0,<8,>=3.7.0, but you'll have importlib-metadata 8.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.15.0 has requirement numpy<2, but you'll have numpy 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-connector-python 3.6.0 has requirement urllib3<2.0.0,>=1.21.1; python_version < \"3.10\", but you'll have urllib3 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fosforio 1.0.2 has requirement pandas==2.0.0, but you'll have pandas 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, numpy, threadpoolctl, joblib, scipy, scikit-learn, xgboost, stopit, pytz, tzdata, six, python-dateutil, pandas, deap, certifi, charset-normalizer, urllib3, idna, requests, update-checker, tpot, cloudpickle, click, blinker, MarkupSafe, Jinja2, zipp, importlib-metadata, itsdangerous, Werkzeug, Flask, querystring-parser, aniso8601, graphql-core, graphql-relay, graphene, pyarrow, Mako, typing-extensions, greenlet, sqlalchemy, alembic, markdown, packaging, gunicorn, wrapt, deprecated, opentelemetry-api, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, databricks-sdk, protobuf, entrypoints, pyyaml, sqlparse, smmap, gitdb, gitpython, opentelemetry-semantic-conventions, opentelemetry-sdk, mlflow-skinny, contourpy, importlib-resources, pillow, cycler, fonttools, kiwisolver, pyparsing, matplotlib, docker, mlflow, pycparser, cffi, cryptography, pyOpenSSL, sortedcontainers, tomlkit, filelock, platformdirs, asn1crypto, pyjwt, snowflake-connector-python, fosforio\n",
      "Successfully installed Flask-3.0.3 Jinja2-3.1.4 Mako-1.3.5 MarkupSafe-2.1.5 Werkzeug-3.0.3 alembic-1.13.2 aniso8601-9.0.1 asn1crypto-1.5.1 blinker-1.8.2 cachetools-5.4.0 certifi-2024.7.4 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 contourpy-1.2.1 cryptography-41.0.7 cycler-0.12.1 databricks-sdk-0.29.0 deap-1.4.1 deprecated-1.2.14 docker-7.1.0 entrypoints-0.4 filelock-3.15.4 fonttools-4.53.1 fosforio-1.0.2 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.32.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 greenlet-3.0.3 gunicorn-22.0.0 idna-3.7 importlib-metadata-8.2.0 importlib-resources-6.4.0 itsdangerous-2.2.0 joblib-1.4.2 kiwisolver-1.4.5 markdown-3.6 matplotlib-3.9.0 mlflow-2.15.0 mlflow-skinny-2.15.0 numpy-2.0.1 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 packaging-24.1 pandas-2.2.2 pillow-10.4.0 platformdirs-3.11.0 protobuf-5.27.3 pyOpenSSL-23.3.0 pyarrow-15.0.2 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycparser-2.22 pyjwt-2.9.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0.1 querystring-parser-1.2.4 requests-2.32.3 rsa-4.9 scikit-learn-1.5.1 scipy-1.13.1 six-1.16.0 smmap-5.0.1 snowflake-connector-python-3.6.0 sortedcontainers-2.4.0 sqlalchemy-2.0.32 sqlparse-0.5.1 stopit-1.1.2 threadpoolctl-3.5.0 tomlkit-0.13.0 tpot-0.12.2 tqdm-4.66.5 typing-extensions-4.12.2 tzdata-2024.1 update-checker-0.18.0 urllib3-2.2.2 wrapt-1.16.0 xgboost-2.1.1 zipp-3.19.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm-4.66.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-2.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.13.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.5.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost-2.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/stopit already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/stopit-1.1.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pandas-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/deap already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/deap-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2024.7.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/update_checker.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/update_checker_test.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/update_checker-0.18.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tpot already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/TPOT-0.12.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot cloudpickle mlflow fosforio[snowflake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab10dc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/ioloop.py\", line 699, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, f))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 825, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 786, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-c1918af9dedb>\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/tmp/pip_packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/tmp/pip_packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/tmp/pip_packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/tmp/pip_packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/ioloop.py\", line 699, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, f))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 825, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 786, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-c1918af9dedb>\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/tmp/pip_packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/tmp/pip_packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/tmp/pip_packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/tmp/pip_packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n",
      "Starting Experiment Execution with the following params:\n",
      "null\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 292\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Running Experiment with user configured params.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Experiment Execution with the following params:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXPERIMENT_DETAILS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPERIMENT_DETAILS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpot_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m           \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity)\u001b[0m\n\u001b[1;32m     34\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_TRACKING_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://mlflow-server\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Setting experiment name\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(\u001b[43mexp_details\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Adding description to the experiment\u001b[39;00m\n\u001b[1;32m     40\u001b[0m tags \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlflow.note.content\u001b[39m\u001b[38;5;124m'\u001b[39m: exp_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_description\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from fosforio import get_local_dataframe, get_dataframe\n",
    "\n",
    "\n",
    "def experiment(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity):\n",
    "    \"\"\"\n",
    "    exp_details: {\n",
    "        name: exp_name\n",
    "        id: exp_id\n",
    "        description: exp_description\n",
    "        dataset: exp_dataset\n",
    "        target_column: exp_target_column\n",
    "        algo_details: None\n",
    "    }\n",
    "    tpot_config: dict\n",
    "    generations: 0.5\n",
    "    population_size: 0.5\n",
    "    cv: 5\n",
    "    random_state: 42\n",
    "    verbosity: 2\n",
    "    \"\"\"\n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "\n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(exp_details.get(\"name\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': exp_details.get(\"description\", \"sample_description\")}\n",
    "\n",
    "    # Reading input data\n",
    "    try:\n",
    "        if exp_details.get(\"dataset\").split(\".\")[-1].lower() in [\"csv\", \"tsv\", \"xlsx\", \"xls\"]:\n",
    "            # Read the input data file from /data mount attached to this notebook pod\n",
    "            input_file = \"/data/\" + exp_details.get(\"dataset\")\n",
    "            data = get_local_dataframe(input_file)\n",
    "        else:\n",
    "            # Input data is a dataset\n",
    "            data = get_dataframe(exp_details.get(\"dataset\"))\n",
    "            # Adding this to log input data in mlflow\n",
    "            input_file = '/tmp/input_data.csv'\n",
    "            data.to_csv(input_file, index=False)            \n",
    "    except Exception as ex:\n",
    "        print(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "\n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    data = data.fillna(method='pad')  # Filling null values with the previous ones\n",
    "    data = data.fillna(method='bfill')  # Filling null value with the next ones\n",
    "    df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature = encoding(\n",
    "        data, exp_details.get(\"target_column\"))\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_file)\n",
    "\n",
    "    for algo, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        if not hyperparam and type(hyperparam) != dict:\n",
    "            hyperparam = TPOTRegressor.default_config_dict[algo]\n",
    "        print(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        # Adding individual algorithm and its hyperparam to tpot config along with preprocessors and selectors\n",
    "        tpot_config_dict = {**tpot_config, **{algo: hyperparam}}\n",
    "        with mlflow.start_run(tags=tags):\n",
    "            pipeline_optimizer = TPOTRegressor(\n",
    "                generations=generations,\n",
    "                population_size=population_size,\n",
    "                cv=cv,\n",
    "                random_state=random_state,\n",
    "                verbosity=verbosity,\n",
    "                config_dict=tpot_config_dict\n",
    "            )\n",
    "\n",
    "            pipeline_optimizer.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = pipeline_optimizer.predict(test_x)\n",
    "\n",
    "            matrices = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            print(f\"{str(pipeline_optimizer.fitted_pipeline_)} pipeline best selected by TPOT\")\n",
    "            print(f\"  score: {pipeline_optimizer.score(test_x, test_y)}\")\n",
    "\n",
    "            try:\n",
    "                # logging hyper params for the best run/pipeline chosen\n",
    "                for step, _ in pipeline_optimizer.fitted_pipeline_.named_steps.items():\n",
    "                    # Checking if the step used in TPOT pipeline is present in TPOT default config dict.\n",
    "                    # if yes, then log only the hyperparams which are present in TPOT default config and not all.\n",
    "                    step_name = [s for s in TPOTRegressor.default_config_dict.keys() if step.lower() in s.lower()]\n",
    "                    for k, v in pipeline_optimizer.fitted_pipeline_.named_steps[step].get_params().items():\n",
    "                        if step_name:\n",
    "                            if k in TPOTRegressor.default_config_dict.get(step_name[0]):\n",
    "                                mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "                        else:\n",
    "                            mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "            except Exception as ex:\n",
    "                print(f\"Exception occurred in logging params to mlflow.\\nex: {ex}\")\n",
    "\n",
    "            # logging model metric\n",
    "            for i in matrices:\n",
    "                if matrices[i]:\n",
    "                    mlflow.log_metric(i, matrices[i])\n",
    "                    print(i, matrices[i])\n",
    "            mlflow.log_metric(\"score\", pipeline_optimizer.score(test_x, test_y))\n",
    "\n",
    "            # Log input data to MLflow run artifact.\n",
    "            mlflow.log_artifact(input_file)\n",
    "\n",
    "            # Registering datasets with mlflow experiment run\n",
    "            mlflow.log_input(dataset, context=\"input\")\n",
    "\n",
    "            # Set custom tags\n",
    "            mlflow.set_tags({\n",
    "                \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "                \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "                \"algorithm\": algo,\n",
    "                \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                \"tpot_selected_pipeline\": str(pipeline_optimizer.fitted_pipeline_)\n",
    "            })\n",
    "\n",
    "            predictions = pipeline_optimizer.fitted_pipeline_.predict(train_x)\n",
    "            signature = infer_signature(train_x, predictions)\n",
    "\n",
    "            # Storing score function for the model\n",
    "            score_and_dump_func(\"/tmp/scoring_func\")\n",
    "            mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "\n",
    "            # Register the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline_optimizer.fitted_pipeline_, \"model\",\n",
    "                registered_model_name=exp_details.get(\"name\", \"sample_experiment\"), signature=signature,\n",
    "                pip_requirements=['mlflow==2.10.0', 'sqlalchemy==1.3.5']\n",
    "            )\n",
    "\n",
    "            # Exporting the autogenerated code of tpot for best pipeline\n",
    "            pipeline_optimizer.export(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "            mlflow.log_artifact(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "\n",
    "\n",
    "def try_or(fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"rmse\": try_or(lambda: np.sqrt(mean_squared_error(actual, pred))),\n",
    "        \"mae\": try_or(lambda: mean_absolute_error(actual, pred)),\n",
    "        \"r2\": try_or(lambda: r2_score(actual, pred))\n",
    "    }\n",
    "\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "\n",
    "        score_output = \"Success\"\n",
    "        return score_output\n",
    "\n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "\n",
    "\n",
    "def encoding(df, target_column):\n",
    "    \"\"\"\n",
    "    Checking whether encoding required in target and feature datasets.\n",
    "    If required, then encoding them with label and one hot encoding.\n",
    "    :param:\n",
    "    df: input dataframe\n",
    "    target_column: target column\n",
    "    :returns:\n",
    "    df_target: target dataframe\n",
    "    le_target: target label encoder object\n",
    "    df_feature: feature dataframe\n",
    "    le_dict_feature: dict of feature label encoder objects\n",
    "    oh_enc_feature: feature one hot encoder object\n",
    "    le_column_feature: list of feature label encoder columns\n",
    "    oh_column_feature: list of feature one hot encoder columns\n",
    "    \"\"\"\n",
    "    df_target = df[[target_column]]\n",
    "    le_target = None\n",
    "    # Target column validation and encoding\n",
    "    if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "        print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "        le_target = LabelEncoder()\n",
    "        df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "        print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "\n",
    "    # Feature column validation and encoding\n",
    "    df_feature = df.drop(target_column, axis=1)\n",
    "    non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    le_dict_feature = {}\n",
    "    le_column_feature = []\n",
    "    oh_column_feature = []\n",
    "    oh_enc_feature = None\n",
    "    if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "\n",
    "\n",
    "# Adding Preprocessors and Selectors with tpot default hyperparameter tuning\n",
    "preprocessors_selectors = [\n",
    "    # Preprocessors\n",
    "    \"sklearn.preprocessing.Binarizer\",\n",
    "    \"sklearn.decomposition.FastICA\",\n",
    "    \"sklearn.cluster.FeatureAgglomeration\",\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\",\n",
    "    \"sklearn.preprocessing.MinMaxScaler\",\n",
    "    \"sklearn.preprocessing.Normalizer\",\n",
    "    \"sklearn.kernel_approximation.Nystroem\",\n",
    "    \"sklearn.decomposition.PCA\",\n",
    "    \"sklearn.preprocessing.PolynomialFeatures\",\n",
    "    \"sklearn.kernel_approximation.RBFSampler\",\n",
    "    \"sklearn.preprocessing.RobustScaler\",\n",
    "    \"sklearn.preprocessing.StandardScaler\",\n",
    "    \"tpot.builtins.ZeroCount\",\n",
    "    \"tpot.builtins.OneHotEncoder\",\n",
    "    # Selectors\n",
    "    \"sklearn.feature_selection.SelectFwe\",\n",
    "    \"sklearn.feature_selection.SelectPercentile\",\n",
    "    \"sklearn.feature_selection.VarianceThreshold\",\n",
    "    \"sklearn.feature_selection.SelectFromModel\"\n",
    "]\n",
    "tpot_config = {key: TPOTRegressor.default_config_dict[key] for key in preprocessors_selectors}\n",
    "\n",
    "# Running Experiment with user configured params.\n",
    "print(f\"Starting Experiment Execution with the following params:\\n{os.getenv('EXPERIMENT_DETAILS')}\\n\")\n",
    "experiment(exp_details=json.loads(os.getenv(\"EXPERIMENT_DETAILS\")), tpot_config=tpot_config, generations=5,\n",
    "           population_size=20, cv=5, random_state=42, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad75ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
